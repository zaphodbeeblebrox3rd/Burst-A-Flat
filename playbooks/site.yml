---
- name: Configure Slurm Cluster
  hosts: all
  become: yes
  gather_facts: yes
    
  pre_tasks:
    - name: Fix broken Ubuntu mirrors
      ansible.builtin.replace:
        path: /etc/apt/sources.list
        regexp: 'mirrors\.edge\.kernel\.org'
        replace: 'archive.ubuntu.com'
      become: yes

    - name: Update package cache and upgrade packages
      ansible.builtin.apt:
        update_cache: yes
        upgrade: yes
        autoremove: yes
      retries: 3
      delay: 90

  roles:
    - role: munge
    - role: common
    - role: slurm

# Note: SlurmDB must be configured before Slurm Controller to avoid dependency issues

- name: Configure MariaDB for SlurmDB
  hosts: slurmdb
  become: yes
  roles:
    - role: mariadb_server
    - role: slurmdb

- name: Configure Slurm Controller
  hosts: slurm_controllers
  become: yes
  roles:
    - role: slurm_controller

- name: Configure Slurm Compute Nodes
  hosts: slurm_computes
  become: yes
  roles:
    - role: slurm_compute
    - role: r_environment

- name: Configure NFS Server
  hosts: nfs_servers
  become: yes
  roles:
    - role: nfs_server

- name: Configure Router
  hosts: management-node
  become: yes
  roles:
    - role: router

- name: Configure Network Routes
  hosts: all
  become: yes
  roles:
    - role: network_config

- name: Configure NFS Clients
  hosts: nfs_clients
  become: yes
  roles:
    - role: nfs_client

- name: Configure R Environment on Login Node
  hosts: slurm_login
  become: yes
  roles:
    - role: common
    - role: r_environment

- name: Configure MongoDB Primary
  hosts: mongodb_primary
  become: yes
  roles:
    - role: mongodb_primary

- name: Configure MongoDB Replica
  hosts: mongodb_replica
  become: yes
  roles:
    - role: mongodb_replica

- name: Final cluster configuration
  hosts: all
  become: yes
  tasks:
    - name: Generate shared SSH key pair for internal node communication
      ansible.builtin.shell: |
        ssh-keygen -t ed25519 -f /home/vagrant/.ssh/id_ed25519_internal -N "" -C "internal-cluster-key"
      args:
        creates: /home/vagrant/.ssh/id_ed25519_internal
      when: inventory_hostname == groups['slurm_controllers'][0]

    - name: Set proper permissions on shared SSH key files
      ansible.builtin.file:
        path: "{{ item }}"
        owner: vagrant
        group: vagrant
        mode: "{{ '0600' if item.endswith('id_ed25519_internal') else '0644' }}"
      loop:
        - /home/vagrant/.ssh/id_ed25519_internal
        - /home/vagrant/.ssh/id_ed25519_internal.pub
      when: inventory_hostname == groups['slurm_controllers'][0]

    - name: Copy shared SSH keys to all nodes
      ansible.builtin.copy:
        src: "{{ item.src }}"
        dest: "{{ item.dest }}"
        owner: vagrant
        group: vagrant
        mode: "{{ item.mode }}"
        remote_src: yes
      loop:
        - { src: "/home/vagrant/.ssh/id_ed25519_internal", dest: "/home/vagrant/.ssh/id_ed25519_internal", mode: "0600" }
        - { src: "/home/vagrant/.ssh/id_ed25519_internal.pub", dest: "/home/vagrant/.ssh/id_ed25519_internal.pub", mode: "0644" }
      when: inventory_hostname != groups['slurm_controllers'][0]
      delegate_to: "{{ groups['slurm_controllers'][0] }}"

    - name: Read shared public key content for authorized_keys
      ansible.builtin.shell: |
        cat /home/vagrant/.ssh/id_ed25519_internal.pub
      delegate_to: "{{ groups['slurm_controllers'][0] }}"
      register: shared_public_key

    - name: Set shared public key fact for all hosts
      ansible.builtin.set_fact:
        shared_public_key_content: "{{ shared_public_key.stdout }}"

    - name: Add shared public key to authorized_keys on all nodes
      ansible.builtin.authorized_key:
        user: vagrant
        key: "{{ shared_public_key_content }}"
        state: present

    - name: Configure SSH to use internal key by default
      ansible.builtin.blockinfile:
        path: /home/vagrant/.ssh/config
        create: yes
        owner: vagrant
        group: vagrant
        mode: '0600'
        block: |
          Host *
            IdentityFile ~/.ssh/id_ed25519_internal
            StrictHostKeyChecking no
            UserKnownHostsFile /dev/null

    - name: Start and enable Slurm services
      ansible.builtin.systemd:
        name: "{{ item }}"
        state: started
        enabled: yes
      loop: "{{ slurm_services }}"
      vars:
        slurm_services: >-
          {%- if inventory_hostname in groups['slurm_controllers'] -%}
            {{ ['munge', 'slurmctld'] }}
          {%- elif inventory_hostname in groups['slurm_computes'] -%}
            {{ ['munge', 'slurmd'] }}
          {%- else -%}
            {{ [] }}
          {%- endif -%}
      when: slurm_services | length > 0

    - name: Create home directory structure for remote nodes without NFS
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: vagrant
        group: vagrant
        mode: '0755'
      loop:
        - /home/vagrant/shared
        - /home/vagrant/shared/scripts
        - /home/vagrant/shared/results
      when: inventory_hostname in groups['network2']

    - name: Create shared home directory structure for NFS
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: vagrant
        group: vagrant
        mode: '0755'
      loop:
        - /home/shared
        - /home/shared/data
        - /home/shared/scripts
        - /home/shared/results
      when: inventory_hostname in groups['slurm_controllers']

    - name: Copy sample data generation script to NFS share
      ansible.builtin.copy:
        src: "{{ playbook_dir }}/../scripts/generate_sample_data.R"
        dest: /home/shared/scripts/generate_sample_data.R
        owner: vagrant
        group: vagrant
        mode: '0755'
      when: inventory_hostname in groups['slurm_controllers']

    - name: Copy R workload demo sbatch script to remote nodes without NFS
      ansible.builtin.copy:
        src: "{{ playbook_dir }}/../scripts/r_workload_demo.sh"
        dest: /home/vagrant/shared/scripts/r_workload_demo.sh
        owner: vagrant
        group: vagrant
        mode: '0755'
      when: inventory_hostname in groups['network2']

    - name: Copy R workload demo sbatch script to NFS share
      ansible.builtin.copy:
        src: "{{ playbook_dir }}/../scripts/r_workload_demo.sh"
        dest: /home/shared/scripts/r_workload_demo.sh
        owner: vagrant
        group: vagrant
        mode: '0755'
      when: inventory_hostname in groups['slurm_controllers']

    - name: Copy R workload demo R script to remote nodes without NFS
      ansible.builtin.copy:
        src: "{{ playbook_dir }}/../scripts/r_workload_demo.R"
        dest: /home/vagrant/shared/scripts/r_workload_demo.R
        owner: vagrant
        group: vagrant
        mode: '0644'
      when: inventory_hostname in groups['network2'] and inventory_hostname not in groups['mongodb_replica']

    - name: Copy R workload demo R script to NFS share
      ansible.builtin.copy:
        src: "{{ playbook_dir }}/../scripts/r_workload_demo.R"
        dest: /home/shared/scripts/r_workload_demo.R
        owner: vagrant
        group: vagrant
        mode: '0644'
      when: inventory_hostname in groups['slurm_controllers']

    - name: Generate sample data on NFS share
      ansible.builtin.shell: |
        cd /home/vagrant/shared/scripts
        Rscript generate_sample_data.R
      become_user: vagrant
      when: inventory_hostname in groups['slurm_login']
      register: sample_data_result
      changed_when: sample_data_result.rc == 0

    - name: Display sample data generation results
      ansible.builtin.debug:
        msg: "Sample data generated successfully in /home/vagrant/shared/data/"
      when:
        - inventory_hostname in groups['slurm_login']
        - sample_data_result.rc == 0

    - name: Wait for Slurm controller to be ready
      ansible.builtin.wait_for:
        port: 6817
        host: "{{ hostvars[groups['slurm_controllers'][0]]['ansible_host'] }}"
        timeout: 30
      delegate_to: "{{ groups['slurm_login'][0] }}"

    - name: Test controller connectivity from login node
      ansible.builtin.shell: |
        ping -c 3 {{ hostvars[groups['slurm_controllers'][0]]['ansible_host'] }}
      register: controller_ping
      delegate_to: "{{ groups['slurm_login'][0] }}"
      failed_when: controller_ping.rc != 0

    - name: Test controller port connectivity
      ansible.builtin.wait_for:
        port: 6817
        host: "{{ hostvars[groups['slurm_controllers'][0]]['ansible_host'] }}"
        timeout: 10
      delegate_to: "{{ groups['slurm_login'][0] }}"

    - name: Check controller service status
      ansible.builtin.shell: |
        systemctl status slurmctld
      register: controller_status
      delegate_to: "{{ groups['slurm_controllers'][0] }}"
      failed_when: false

    - name: Debug controller status
      ansible.builtin.debug:
        var: controller_status.stdout

    - name: Check if slurmctld is actually running
      ansible.builtin.shell: |
        ps aux | grep slurmctld | grep -v grep
      register: slurmctld_process
      delegate_to: "{{ groups['slurm_controllers'][0] }}"
      failed_when: false

    - name: Debug slurmctld process
      ansible.builtin.debug:
        var: slurmctld_process.stdout

    - name: Check what's listening on port 6817
      ansible.builtin.shell: |
        netstat -tlnp | grep 6817
      register: port_6817_check
      delegate_to: "{{ groups['slurm_controllers'][0] }}"
      failed_when: false

    - name: Debug port 6817
      ansible.builtin.debug:
        var: port_6817_check.stdout

    - name: Check slurmctld logs for errors
      ansible.builtin.shell: |
        tail -20 /var/log/slurm/slurmctld.log
      register: slurmctld_logs
      delegate_to: "{{ groups['slurm_controllers'][0] }}"
      failed_when: false

    - name: Debug slurmctld logs
      ansible.builtin.debug:
        var: slurmctld_logs.stdout

    - name: Check slurm.conf on controller
      ansible.builtin.shell: |
        cat /etc/slurm/slurm.conf | grep -E "(ControlMachine|ControlAddr|ClusterName)"
      register: controller_slurm_conf
      delegate_to: "{{ groups['slurm_controllers'][0] }}"
      failed_when: false

    - name: Debug controller slurm.conf
      ansible.builtin.debug:
        var: controller_slurm_conf.stdout

    - name: Check slurm.conf on login node
      ansible.builtin.shell: |
        cat /etc/slurm/slurm.conf | grep -E "(ControlMachine|ControlAddr|ClusterName)"
      register: login_slurm_conf
      delegate_to: "{{ groups['slurm_login'][0] }}"
      failed_when: false

    - name: Debug login slurm.conf
      ansible.builtin.debug:
        var: login_slurm_conf.stdout

    - name: Check Munge key on controller
      ansible.builtin.shell: |
        md5sum /etc/munge/munge.key
      register: controller_munge_key
      delegate_to: "{{ groups['slurm_controllers'][0] }}"
      failed_when: false

    - name: Debug controller Munge key
      ansible.builtin.debug:
        var: controller_munge_key.stdout

    - name: Check Munge key on login node
      ansible.builtin.shell: |
        md5sum /etc/munge/munge.key
      register: login_munge_key
      delegate_to: "{{ groups['slurm_login'][0] }}"
      failed_when: false

    - name: Debug login Munge key
      ansible.builtin.debug:
        var: login_munge_key.stdout

    - name: Restart Munge service on all Slurm nodes
      ansible.builtin.systemd:
        name: munge
        state: restarted
      when: inventory_hostname in groups['slurm_controllers'] or inventory_hostname in groups['slurm_computes'] or inventory_hostname in groups['slurm_login']

    - name: Restart Slurm controller service
      ansible.builtin.systemd:
        name: slurmctld
        state: restarted
      when: inventory_hostname in groups['slurm_controllers']

    - name: Restart Slurm compute daemon service
      ansible.builtin.systemd:
        name: slurmd
        state: restarted
      when: inventory_hostname in groups['slurm_computes']


    - name: Check Slurm configuration on login node
      ansible.builtin.shell: |
        cat /etc/slurm/slurm.conf | grep -E "(ControlMachine|ControlAddr|ClusterName)"
      register: slurm_config
      delegate_to: "{{ groups['slurm_login'][0] }}"
      failed_when: false

    - name: Debug Slurm configuration
      ansible.builtin.debug:
        var: slurm_config.stdout

    - name: Test Slurm controller connection with verbose output
      ansible.builtin.shell: |
        scontrol show config | head -20
      register: scontrol_test
      delegate_to: "{{ groups['slurm_login'][0] }}"
      failed_when: false

    - name: Debug scontrol output
      ansible.builtin.debug:
        var: scontrol_test.stdout

    - name: Validate Slurm cluster is operational
      ansible.builtin.shell: |
        sinfo --format="%.10N %.5D %.6t %.10M %.6D %R" --noheader
      register: sinfo_output
      delegate_to: "{{ groups['slurm_login'][0] }}"
      failed_when: sinfo_output.rc != 0
      changed_when: false
      retries: 3
      delay: 10

    - name: Check for NOT_RESPONDING nodes
      ansible.builtin.shell: |
        sinfo --format="%.6t" --noheader | grep -q "NOT_RESPONDING" && echo "NOT_RESPONDING" || echo "OK"
      register: node_status_check
      delegate_to: "{{ groups['slurm_login'][0] }}"
      failed_when: false
      changed_when: false

    - name: Fail if nodes are not responding
      ansible.builtin.fail:
        msg: "Some compute nodes are not responding. Check munge authentication and node connectivity."
      when: node_status_check.stdout == "NOT_RESPONDING"

    - name: Display cluster validation results
      ansible.builtin.debug:
        msg: |
          ✅ Slurm cluster validation successful!
          
          Cluster Status:
          {{ sinfo_output.stdout }}
          
          Cluster setup complete!
          Login to the cluster: vagrant ssh login-node
          Check cluster status: sinfo
          Submit a job: sbatch /home/vagrant/shared/scripts/r_workload_demo.sh
